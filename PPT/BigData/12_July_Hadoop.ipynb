{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17c27a-abad-4a09-b8bd-7c49291effb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Read and display the core components of Hadoop from a configuration file:\n",
    "def read_hadoop_config(config_file):\n",
    "    core_components = []\n",
    "    with open(config_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"<name>\"):\n",
    "                component = line.strip(\"<name>\").strip(\"</>\").strip()\n",
    "                core_components.append(component)\n",
    "    return core_components\n",
    "\n",
    "config_file = \"hadoop-config.xml\"\n",
    "core_components = read_hadoop_config(config_file)\n",
    "print(\"Core components of Hadoop:\")\n",
    "for component in core_components:\n",
    "    print(component)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022366df-b8f7-427c-952b-0d703b1056a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Calculate the total file size in a Hadoop Distributed File System (HDFS) directory:\n",
    "import subprocess\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    cmd = f\"hadoop fs -du -s {directory}\"\n",
    "    output = subprocess.check_output(cmd, shell=True)\n",
    "    total_size = int(output.decode().split()[0])\n",
    "    return total_size\n",
    "\n",
    "hdfs_directory = \"/user/data\"\n",
    "total_size = get_directory_size(hdfs_directory)\n",
    "print(f\"Total file size in {hdfs_directory}: {total_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab505737-f4f6-42c6-b465-fd79ece75477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Extract and display the top N most frequent words from a large text file using the MapReduce approach:\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def get_top_words(text_file, n):\n",
    "    sc = SparkContext(\"local\", \"WordCount\")\n",
    "    text = sc.textFile(text_file)\n",
    "    word_counts = text.flatMap(lambda line: line.split(\" \")) \\\n",
    "                     .map(lambda word: (word, 1)) \\\n",
    "                     .reduceByKey(lambda a, b: a + b)\n",
    "    top_words = word_counts.takeOrdered(n, key=lambda x: -x[1])\n",
    "    return top_words\n",
    "\n",
    "text_file = \"large_text.txt\"\n",
    "n = 10\n",
    "top_words = get_top_words(text_file, n)\n",
    "print(f\"Top {n} most frequent words:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc375b-7b62-4ea8-8e99-9ea6793a38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Check the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API:\n",
    "import requests\n",
    "\n",
    "def check_health_status():\n",
    "    namenode_url = \"http://namenode:50070/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
    "    datanode_url = \"http://datanode:50075/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "    \n",
    "    namenode_response = requests.get(namenode_url).json()\n",
    "    namenode_status = namenode_response['beans'][0]['State']\n",
    "    \n",
    "    datanode_response = requests.get(datanode_url).json()\n",
    "    datanode_status = datanode_response['beans'][0]['State']\n",
    "    \n",
    "    return namenode_status, datanode_status\n",
    "\n",
    "nn_status, dn_status = check_health_status()\n",
    "print(\"Health Status:\")\n",
    "print(f\"NameNode: {nn_status}\")\n",
    "print(f\"DataNode: {dn_status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9fdeb-e161-4398-a38e-8e3bab947b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.List all the files and directories in a specific HDFS path:\n",
    "import subprocess\n",
    "\n",
    "def list_hdfs_path(hdfs_path):\n",
    "    cmd = f\"hadoop fs -ls {hdfs_path}\"\n",
    "    output = subprocess.check_output(cmd, shell=True)\n",
    "    files = output.decode().split(\"\\n\")[1:-1]\n",
    "    file_list = [file.split()[-1] for file in files]\n",
    "    return file_list\n",
    "\n",
    "hdfs_path = \"/user/data\"\n",
    "files = list_hdfs_path(hdfs_path)\n",
    "print(\"Files and directories:\")\n",
    "for file in files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374a515-4b5d-443e-8217-666f2390877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Analyze the storage utilization of DataNodes in a Hadoop cluster and identify the nodes with the highest and lowest storage capacities:\n",
    "import requests\n",
    "\n",
    "def analyze_data_nodes():\n",
    "    datanodes_url = \"http://datanode:50075/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-*\"\n",
    "    datanodes_response = requests.get(datanodes_url).json()\n",
    "    datanodes = datanodes_response['beans']\n",
    "    \n",
    "    storage_utilization = []\n",
    "    for datanode in datanodes:\n",
    "        storage_info = {\n",
    "            'node': datanode['name'].split('=')[-1],\n",
    "            'capacity': datanode['Capacity'],\n",
    "            'used': datanode['DfsUsed'],\n",
    "            'remaining': datanode['Remaining'],\n",
    "            'utilization': float(datanode['DfsUsed']) / float(datanode['Capacity'])\n",
    "        }\n",
    "        storage_utilization.append(storage_info)\n",
    "    \n",
    "    storage_utilization.sort(key=lambda x: x['utilization'], reverse=True)\n",
    "    highest_utilization = storage_utilization[0]\n",
    "    lowest_utilization = storage_utilization[-1]\n",
    "    \n",
    "    return highest_utilization, lowest_utilization\n",
    "\n",
    "highest_util, lowest_util = analyze_data_nodes()\n",
    "print(\"Storage Utilization:\")\n",
    "print(f\"Highest Utilization: {highest_util['node']} ({highest_util['utilization']*100:.2f}%)\")\n",
    "print(f\"Lowest Utilization: {lowest_util['node']} ({lowest_util['utilization']*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc614e7e-5e4d-46da-9b5f-0a163fd11e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.Interact with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output:\n",
    "import requests\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path):\n",
    "    url = \"http://resourcemanager:8088/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(url)\n",
    "    application_id = response.json()['application-id']\n",
    "    \n",
    "    submit_url = f\"http://resourcemanager:8088/ws/v1/cluster/apps/{application_id}/submit\"\n",
    "    submit_payload = {\n",
    "        \"application-id\": application_id,\n",
    "        \"application-name\": \"MyHadoopJob\",\n",
    "        \"am-container-spec\": {\n",
    "            \"commands\": {\n",
    "                \"command\": f\"hadoop jar {jar_path} input {output_path}\"\n",
    "            },\n",
    "            \"local-resources\": {\n",
    "                \"resource\": [\n",
    "                    {\n",
    "                        \"name\": \"input\",\n",
    "                        \"type\": \"FILE\",\n",
    "                        \"visibility\": \"APPLICATION\",\n",
    "                        \"uri\": f\"file://{input_path}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"unmanaged-AM\": False\n",
    "    }\n",
    "    requests.post(submit_url, json=submit_payload)\n",
    "    \n",
    "    return application_id\n",
    "\n",
    "def monitor_job_progress(application_id):\n",
    "    url = f\"http://resourcemanager:8088/ws/v1/cluster/apps/{application_id}\"\n",
    "    response = requests.get(url)\n",
    "    state = response.json()['app']['state']\n",
    "    final_status = response.json()['app']['finalStatus']\n",
    "    \n",
    "    return state, final_status\n",
    "\n",
    "def retrieve_job_output(output_path):\n",
    "    url = f\"http://resourcemanager:8088/ws/v1/cluster/apps/{application_id}/containers\"\n",
    "    response = requests.get(url)\n",
    "    containers = response.json()['containers']\n",
    "    container_id = containers[0]['id']\n",
    "    \n",
    "    output_url = f\"http://node:8042/node/containerlogs/{container_id}/stdout\"\n",
    "    response = requests.get(output_url)\n",
    "    job_output = response.text\n",
    "    \n",
    "    return job_output\n",
    "\n",
    "jar_path = \"myjob.jar\"\n",
    "input_path = \"input.txt\"\n",
    "output_path = \"output\"\n",
    "application_id = submit_hadoop_job(jar_path, input_path, output_path)\n",
    "state, final_status = monitor_job_progress(application_id)\n",
    "job_output = retrieve_job_output(output_path)\n",
    "\n",
    "print(\"Job Progress:\")\n",
    "print(f\"Application ID: {application_id}\")\n",
    "print(f\"State: {state}\")\n",
    "print(f\"Final Status: {final_status}\")\n",
    "\n",
    "print(\"Job Output:\")\n",
    "print(job_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f005a53-9ff7-44ba-bb41-d205b23382a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Interact with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution:\n",
    "import requests\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path, num_executors, executor_memory):\n",
    "    url = \"http://resourcemanager:8088/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(url)\n",
    "    application_id = response.json()['application-id']\n",
    "    \n",
    "    submit_url = f\"http://resourcemanager:8088/ws/v1/cluster/apps/{application_id}/submit\"\n",
    "    submit_payload = {\n",
    "        \"application-id\": application_id,\n",
    "        \"application-name\": \"MyHadoopJob\",\n",
    "        \"am-container-spec\": {\n",
    "            \"commands\": {\n",
    "                \"command\": f\"hadoop jar {jar_path} input {output_path}\"\n",
    "            },\n",
    "            \"local-resources\": {\n",
    "                \"resource\": [\n",
    "                    {\n",
    "                        \"name\": \"input\",\n",
    "                        \"type\": \"FILE\",\n",
    "                        \"visibility\": \"APPLICATION\",\n",
    "                        \"uri\": f\"file://{input_path}\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"resource\": {\n",
    "                \"vCores\": 1,\n",
    "                \"memory\": 1024\n",
    "            },\n",
    "            \"instances\": 1,\n",
    "            \"environment\": {\n",
    "                \"variables\": {\n",
    "                    \"SPARK_EXECUTOR_MEMORY\": executor_memory\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"unmanaged-AM\": False\n",
    "    }\n",
    "    requests.post(submit_url, json=submit_payload)\n",
    "    \n",
    "    return application_id\n",
    "\n",
    "def monitor_resource_usage(application_id):\n",
    "    url = f\"http://resourcemanager:8088/ws/v1/cluster/apps/{application_id}/containers\"\n",
    "    response = requests.get(url)\n",
    "    containers = response.json()['containers']\n",
    "    total_memory = 0\n",
    "    total_vcores = 0\n",
    "    \n",
    "    for container in containers:\n",
    "        total_memory += container['allocatedMB']\n",
    "        total_vcores += container['allocatedVCores']\n",
    "    \n",
    "    return total_memory, total_vcores\n",
    "\n",
    "jar_path = \"myjob.jar\"\n",
    "input_path = \"input.txt\"\n",
    "output_path = \"output\"\n",
    "num_executors = 4\n",
    "executor_memory = \"4g\"\n",
    "application_id = submit_hadoop_job(jar_path, input_path, output_path, num_executors, executor_memory)\n",
    "total_memory, total_vcores = monitor_resource_usage(application_id)\n",
    "\n",
    "print(\"Resource Usage:\")\n",
    "print(f\"Application ID: {application_id}\")\n",
    "print(f\"Total Memory Allocated: {total_memory} MB\")\n",
    "print(f\"Total vCores Allocated: {total_vcores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9a35c-1287-4353-a43f-1f6bb2364a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.Compare the performance of a MapReduce job with different input split sizes, showcasing the impact on the overall job execution time:\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_mapreduce_job(input_file, split_size):\n",
    "    start_time = time.time()\n",
    "    cmd = f\"hadoop jar mapreduce.jar input {input_file} split {split_size}\"\n",
    "    subprocess.check_output(cmd, shell=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    return execution_time\n",
    "\n",
    "input_file = \"large_input.txt\"\n",
    "split_sizes = [64, 128, 256]\n",
    "for split_size in split_sizes:\n",
    "    execution_time = run_mapreduce_job(input_file, split_size)\n",
    "    print(f\"Execution time for split size {split_size}: {execution_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
