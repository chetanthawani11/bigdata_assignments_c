{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc11710-4c3d-4a6a-b9b4-7e7949362cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Data Warehousing Fundamentals\n",
    "\n",
    "1. Design a data warehouse schema for a retail company that includes dimension tables for products, customers, and time. Implement the schema using a relational database management system (RDBMS) of your choice.\n",
    "\n",
    "Datawarehouse schema for a retail company:\n",
    "\n",
    "- Product Dimension Table:\n",
    "  - product_id (Primary Key)\n",
    "  - product_name\n",
    "  - category\n",
    "  - brand\n",
    "  - price\n",
    "\n",
    "- Customer Dimension Table:\n",
    "  - customer_id (Primary Key)\n",
    "  - customer_name\n",
    "  - gender\n",
    "  - age\n",
    "  - address\n",
    "\n",
    "- Time Dimension Table:\n",
    "  - date_id (Primary Key)\n",
    "  - date\n",
    "  - day_of_week\n",
    "  - month\n",
    "  - year\n",
    "\n",
    "- Sales Fact Table:\n",
    "  - sales_id (Primary Key)\n",
    "  - product_id (Foreign Key referencing Product Dimension Table)\n",
    "  - customer_id (Foreign Key referencing Customer Dimension Table)\n",
    "  - date_id (Foreign Key referencing Time Dimension Table)\n",
    "  - quantity_sold\n",
    "  - sales_amount\n",
    "\n",
    "# We can implement this schema using an RDBMS such as MySQL, PostgreSQL, or Oracle. Create the necessary tables with appropriate data types and relationships.\n",
    "\n",
    "2. Create a fact table that captures sales data, including product ID, customer ID, date, and sales amount. Populate the fact table with sample data.\n",
    "\n",
    "Using the schema from the previous question, we can create and populate the Sales Fact Table with sample data.\n",
    "\n",
    "```sql\n",
    "INSERT INTO sales_fact_table (product_id, customer_id, date_id, quantity_sold, sales_amount)\n",
    "VALUES\n",
    "  (1, 1, 1, 5, 100.00),\n",
    "  (2, 2, 2, 3, 50.00),\n",
    "  (3, 1, 3, 2, 30.00),\n",
    "  ...\n",
    "  ;\n",
    "```\n",
    "\n",
    "# we can populate the fact table with more sample data according to your requirements.\n",
    "\n",
    "3. Write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions.\n",
    "\n",
    "SQL queries to retrieve sales data from the data warehouse:\n",
    "\n",
    "- Retrieve total sales amount for each product:\n",
    "```sql\n",
    "SELECT p.product_name, SUM(f.sales_amount) AS total_sales_amount\n",
    "FROM sales_fact_table f\n",
    "JOIN product_dimension_table p ON f.product_id = p.product_id\n",
    "GROUP BY p.product_name;\n",
    "```\n",
    "\n",
    "- Retrieve sales amount by month and year:\n",
    "```sql\n",
    "SELECT t.year, t.month, SUM(f.sales_amount) AS monthly_sales_amount\n",
    "FROM sales_fact_table f\n",
    "JOIN time_dimension_table t ON f.date_id = t.date_id\n",
    "GROUP BY t.year, t.month;\n",
    "```\n",
    "\n",
    "- Retrieve sales amount by customer gender:\n",
    "```sql\n",
    "SELECT c.gender, SUM(f.sales_amount) AS sales_amount_by_gender\n",
    "FROM sales_fact_table f\n",
    "JOIN customer_dimension_table c ON f.customer_id = c.customer_id\n",
    "GROUP BY c.gender;\n",
    "```\n",
    "\n",
    "- Retrieve sales amount for a specific product and date range:\n",
    "```sql\n",
    "SELECT p.product_name, t.date, f.sales_amount\n",
    "FROM sales_fact_table f\n",
    "JOIN product_dimension_table p ON f.product_id = p.product_id\n",
    "JOIN time_dimension_table t ON f.date_id = t.date_id\n",
    "WHERE p.product_name = 'Product X' AND t.date BETWEEN '2022-01-01' AND '2022-12-31';\n",
    "```\n",
    "\n",
    "# We can customize the queries based on your specific requirements and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd313e-1d93-4d00-95c2-b71cfe537fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: ETL and Data Integration\n",
    "\n",
    "1. Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
    "\n",
    "# ETL process using Python:\n",
    "\n",
    "- Extraction:\n",
    "  - Read data from the source system (e.g., CSV files) using libraries like pandas or csv.\n",
    "  - Extract relevant data columns and rows based on your business rules.\n",
    "\n",
    "- Transformation:\n",
    "  - Apply business rules, calculations, or data manipulations to the extracted data.\n",
    "  - Clean and format the data as required.\n",
    "  - Perform data quality checks or validations.\n",
    "\n",
    "- Load:\n",
    "  - Connect to the data warehouse (e.g., using a database driver or library).\n",
    "  - Create tables or truncate existing tables in the data warehouse if needed.\n",
    "  - Load the transformed data into the appropriate tables in the data warehouse.\n",
    "\n",
    "2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n",
    "\n",
    "# Python code for extraction, transformation, and loading steps:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import psycopg2  # Assuming PostgreSQL as the data warehouse\n",
    "\n",
    "# Extraction\n",
    "data = pd.read_csv('source_data.csv')\n",
    "extracted_data = data[['column1', 'column2', 'column3']]  # Selecting relevant columns\n",
    "\n",
    "# Transformation\n",
    "transformed_data = extracted_data.apply(lambda x: x * 2)  # Example transformation: multiplying values by 2\n",
    "\n",
    "# Load\n",
    "conn = psycopg2.connect(database=\"your_database\", user=\"your_user\", password=\"your_password\", host=\"your_host\", port=\"your_port\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Truncate existing table (optional)\n",
    "cur.execute(\"TRUNCATE TABLE your_table\")\n",
    "\n",
    "# Insert transformed data into the data warehouse\n",
    "for index, row in transformed_data.iterrows():\n",
    "    cur.execute(\"INSERT INTO your_table (column1, column2, column3) VALUES (%s, %s, %s)\",\n",
    "                (row['column1'], row['column2'], row['column3']))\n",
    "\n",
    "# Commit changes and close the connection\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "```\n",
    "\n",
    "# We will need to modify the code according to your specific source data format, transformation logic, and data warehouse configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb27c8d-15ff-46c8-925e-156e464bb9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Dimensional Modeling and Schemas\n",
    "\n",
    "1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
    "\n",
    "# Star schema for a university database:\n",
    "\n",
    "- Fact Table: Enrollment Fact Table\n",
    "  - enrollment_id (Primary Key)\n",
    "  - student_id (Foreign Key referencing Student Dimension Table)\n",
    "  - course_id (Foreign Key referencing Course Dimension Table)\n",
    "  - time_id (Foreign Key referencing Time Dimension Table)\n",
    "  - grade\n",
    "\n",
    "- Dimension Table: Student Dimension Table\n",
    "  - student_id (Primary Key)\n",
    "  - student_name\n",
    "  - major\n",
    "  - admission_year\n",
    "\n",
    "- Dimension Table: Course Dimension Table\n",
    "  - course_id (Primary Key)\n",
    "  - course_name\n",
    "  - department\n",
    "  - credits\n",
    "\n",
    "- Dimension Table: Time Dimension Table\n",
    "  - time_id (Primary Key)\n",
    "  - semester\n",
    "  - year\n",
    "\n",
    "# We can implement this schema using a database management system of our choice (e.g., MySQL, PostgreSQL, Oracle). Create the necessary tables with appropriate data types and relationships.\n",
    "\n",
    "2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables.\n",
    "\n",
    "# SQL queries to retrieve data from the star schema:\n",
    "\n",
    "- Retrieve the number of enrollments per course:\n",
    "```sql\n",
    "SELECT c.course_name, COUNT(e.enrollment_id) AS enrollments_count\n",
    "FROM enrollment_fact_table e\n",
    "JOIN course_dimension_table c ON e.course_id = c.course_id\n",
    "GROUP BY c.course_name;\n",
    "```\n",
    "\n",
    "- Retrieve the average grade per semester:\n",
    "```sql\n",
    "SELECT t.semester, AVG(e.grade) AS average_grade\n",
    "FROM enrollment_fact_table e\n",
    "JOIN time_dimension_table t ON e.time_id = t.time_id\n",
    "GROUP BY t.semester;\n",
    "```\n",
    "\n",
    "- Retrieve the student details for a specific course:\n",
    "```sql\n",
    "SELECT s.student_name, s.major, e.grade\n",
    "FROM enrollment_fact_table e\n",
    "JOIN student_dimension_table s ON e.student_id = s.student_id\n",
    "WHERE e.course_id = 'COURSE_ID';\n",
    "```\n",
    "\n",
    "- Retrieve the number of enrollments per department in a specific year:\n",
    "```sql\n",
    "SELECT c.department, COUNT(e.enrollment_id) AS enrollments_count\n",
    "FROM enrollment_fact_table e\n",
    "JOIN course_dimension_table c ON e.course_id = c.course_id\n",
    "JOIN time_dimension_table t ON e.time_id = t.time_id\n",
    "WHERE t.year = 'YEAR'\n",
    "GROUP BY c.department;\n",
    "```\n",
    "\n",
    "# We can customize these queries based on your specific requirements and the dimensions in your star schema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f00d9b-1c54-4c71-8773-a9718198ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Performance Optimization and Querying\n",
    "\n",
    "1. Scenario: You need to improve the performance of your data loading process in the data warehouse. Write a Python script that implements the following optimizations:\n",
    "\n",
    "a) Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
    "\n",
    "```python\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Connection details\n",
    "conn = psycopg2.connect(database=\"your_database\", user=\"your_user\", password=\"your_password\", host=\"your_host\", port=\"your_port\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Load data in batches\n",
    "batch_size = 1000\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "for batch_start in range(0, len(data), batch_size):\n",
    "    batch = data.iloc[batch_start:batch_start+batch_size]\n",
    "    values = batch.to_records(index=False)\n",
    "\n",
    "    # Generate SQL statement for batch insertion\n",
    "    query = \"INSERT INTO your_table (column1, column2, ...) VALUES %s\"\n",
    "    psycopg2.extras.execute_values(cur, query, values)\n",
    "\n",
    "# Commit changes and close the connection\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "```\n",
    "\n",
    "b) Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
    "\n",
    "```python\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def load_data_batch(batch):\n",
    "    # Connection details\n",
    "    conn = psycopg2.connect(database=\"your_database\", user=\"your_user\", password=\"your_password\", host=\"your_host\", port=\"your_port\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Generate SQL statement for batch insertion\n",
    "    query = \"INSERT INTO your_table (column1, column2, ...) VALUES %s\"\n",
    "    psycopg2.extras.execute_values(cur, query, batch.to_records(index=False))\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Load data in parallel using multiple threads\n",
    "batch_size = 1000\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    for batch_start in range(0, len(data), batch_size):\n",
    "        batch = data.iloc[batch_start:batch_start+batch_size]\n",
    "        executor.submit(load_data_batch, batch)\n",
    "```\n",
    "\n",
    "c) Measure the time taken to load a specific amount of data before and after implementing these optimizations.\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Your data loading process\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Time taken: {execution_time} seconds\")\n",
    "```\n",
    "\n",
    "# We can Measure the execution time before and after implementing the optimizations to compare the performance improvement.\n",
    "\n",
    "# We can modify the code according to your specific data loading process, database configuration, and requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
